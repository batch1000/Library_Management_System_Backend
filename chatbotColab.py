from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok
from sentence_transformers import SentenceTransformer, util
import torch
import google.generativeai as genai
import os
import requests
import json
import re
import time


BACKEND_URL = "https://libraryphuongb2103514.pagekite.me/api/chatbot"

# Cáº¥u hÃ¬nh Ngrok
!ngrok authtoken 35E1InvyKUj8F2iSLdU6N60wnLM_2DSMWqQTPd7gHXVDH3fC5

# Cáº¥u hÃ¬nh Gemini API (Láº¤Y KEY Táº I: https://makersuite.google.com/app/apikey)
GEMINI_API_KEY = "AIzaSyDkzmIMw2CwvKwa2h4oMG-nNJSkSPUN2kY"  # âš ï¸ THAY KEY Cá»¦A Báº N VÃ€O ÄÃ‚Y
genai.configure(api_key=GEMINI_API_KEY)

# ===========================================
# BÆ¯á»šC 3: KHá»I Táº O MODEL & BIáº¾N TOÃ€N Cá»¤C
# ===========================================
print("â³ Äang load embedding model...")
embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
print("âœ… Embedding model Ä‘Ã£ sáºµn sÃ ng!")

# Biáº¿n lÆ°u trá»¯ database
database_texts = []
database_embeddings = None

# Khá»Ÿi táº¡o Gemini model
gemini_model = genai.GenerativeModel('models/gemini-2.5-flash')

# ===========================================
# BÆ¯á»šC 4: Táº O FLASK APP
# ===========================================
app = Flask(__name__)
CORS(app, supports_credentials=True, resources={r"/*": {"origins": "*"}})

# ===========================================
# HÃ€M PHÃ‚N LOáº I INTENT Má»šI
# ===========================================
def classify_intent(user_message):
    """PhÃ¢n loáº¡i cÃ¢u há»i cá»§a user báº±ng Gemini - CHá»ˆ TIM_SACH_LEVEL_1"""
    prompt = f"""Báº¡n lÃ  AI phÃ¢n loáº¡i cÃ¢u há»i vá» thÆ° viá»‡n.

NHIá»†M Vá»¤: PhÃ¢n tÃ­ch cÃ¢u há»i vÃ  tráº£ vá» JSON format.

CÃ‚U Há»I: "{user_message}"

DANH SÃCH INTENT:
1. "tim_sach_level_1" - TÃ¬m sÃ¡ch ÄÆ N GIáº¢N vá»›i cÃ¡c bá»™ lá»c (tÃªn, tÃ¡c giáº£, thá»ƒ loáº¡i, nÄƒm xuáº¥t báº£n, giÃ¡, rating, top ranking, sáº¯p xáº¿p...)
2. "tim_sach_level_2" - TÃ¬m sÃ¡ch PHá»¨C Táº P vá»›i Ä‘iá»u kiá»‡n AND/OR/NOT (vÃ­ dá»¥: "SÃ¡ch cá»§a A HOáº¶C B", "SÃ¡ch vÄƒn há»c VÃ€ rating cao", "KHÃ”NG pháº£i giÃ¡o trÃ¬nh")
3. "general_info" - ThÃ´ng tin chung vá» thÆ° viá»‡n, quy Ä‘á»‹nh, giá» má»Ÿ cá»­a...

SCHEMA CHO tim_sach_level_1:
{{
  "intent": "tim_sach_level_1",
  "filters": {{
    "TenSach": null,              // TÃªn sÃ¡ch (string hoáº·c null)
    "TacGia": null,               // TÃ¡c giáº£ (string hoáº·c null)
    "TheLoai": null,              // Thá»ƒ loáº¡i (string hoáº·c null)
    "NXB": null,                  // NhÃ  xuáº¥t báº£n (string hoáº·c null)
    "LoaiSach": null,             // "Sach" hoáº·c "GiaoTrinh" hoáº·c null
    "Khoa": null,                 // Khoa (string hoáº·c null)
    "MoTaSach": null,             // MÃ´ táº£ sÃ¡ch (string hoáº·c null)
    "NamXuatBanMin": null,        // NÄƒm xuáº¥t báº£n min (number hoáº·c null)
    "NamXuatBanMax": null,        // NÄƒm xuáº¥t báº£n max (number hoáº·c null)
    "DonGiaMin": null,            // GiÃ¡ sÃ¡ch min (number hoáº·c null)
    "DonGiaMax": null,            // GiÃ¡ sÃ¡ch max (number hoáº·c null)
    "SoSaoMin": null,             // Sá»‘ sao min (number hoáº·c null)
    "SoSaoMax": null              // Sá»‘ sao max (number hoáº·c null)
  }},
  "topList": [
    // Chá»‰ thÃªm náº¿u user há»i top/xáº¿p háº¡ng
    // {{ "field": "LuotMuon", "order": "desc", "limit": 10 }}
    // {{ "field": "LuotXem", "order": "desc", "limit": 10 }}
    // {{ "field": "DanhGia", "order": "desc", "limit": 10 }}
  ],
  "sort": {{
    "field": null,                // Field cáº§n sort (string hoáº·c null)
    "order": null                 // 1=tÄƒng dáº§n, -1=giáº£m dáº§n (number hoáº·c null)
  }},
  "limit": 10,                    // Giá»›i háº¡n sá»‘ káº¿t quáº£ (máº·c Ä‘á»‹nh 10)
  "confidence": 0.9               // Äá»™ tin cáº­y (0.0-1.0)
}}


VÃ Dá»¤ tim_sach_level_2:
- "TÃ¬m sÃ¡ch cá»§a tÃ¡c giáº£ Nguyá»…n Nháº­t Ãnh" â†’ filters.TacGia = "Nguyá»…n Nháº­t Ãnh"
- "Top 5 sÃ¡ch nhiá»u lÆ°á»£t mÆ°á»£n nháº¥t" â†’ topList = [{{ "field": "LuotMuon", "order": "desc", "limit": 5 }}]
- "SÃ¡ch nÄƒm 2020 Ä‘áº¿n 2023" â†’ filters.NamXuatBanMin = 2020, filters.NamXuatBanMax = 2023
- "GiÃ¡o trÃ¬nh khoa CÃ´ng nghá»‡ thÃ´ng tin" â†’ filters.LoaiSach = "GiaoTrinh", filters.Khoa = "CÃ´ng nghá»‡ thÃ´ng tin"
- "SÃ¡ch rating cao nháº¥t" â†’ topList = [{{ "field": "DanhGia", "order": "desc", "limit": 10 }}]
- "SÃ¡ch má»›i nháº¥t" â†’ sort = {{ "field": "createdAt", "order": -1 }}

SCHEMA CHO tim_sach_level_2:
{{
  "intent": "tim_sach_level_2",
  "query": {{
    "operator": "AND",              // "AND" | "OR" | "NOT"
    "conditions": [
      {{
        "field": "TenSach",         // Field cáº§n filter: TenSach, TacGia, TheLoai, NXB, LoaiSach, Khoa, MoTaSach, NamXuatBan, DonGia
        "operator": "contains",     // "contains" | "equals" | "gte" | "lte" | "in"
        "value": "...",             // GiÃ¡ trá»‹ cáº§n tÃ¬m
        "negate": false             // true = NOT condition
      }}
    ],
    "subQueries": [                 // Nested queries cho Ä‘iá»u kiá»‡n phá»©c táº¡p (tÃ¹y chá»n)
      {{
        "operator": "OR",
        "conditions": [...]
      }}
    ]
  }},
  "topList": [                      // TÆ°Æ¡ng tá»± level_1
    // {{ "field": "LuotMuon", "order": "desc", "limit": 10 }}
  ],
  "sort": {{                        // TÆ°Æ¡ng tá»± level_1
    "field": null,
    "order": null
  }},
  "limit": 10,
  "confidence": 0.9
}}

VÃ Dá»¤ tim_sach_level_2:
- "TÃ¬m sÃ¡ch cá»§a Nguyá»…n Nháº­t Ãnh HOáº¶C Paulo Coelho"
  â†’ {{ "operator": "OR", "conditions": [
       {{ "field": "TacGia", "operator": "contains", "value": "Nguyá»…n Nháº­t Ãnh" }},
       {{ "field": "TacGia", "operator": "contains", "value": "Paulo Coelho" }}
     ] }}

- "SÃ¡ch vÄƒn há»c nÄƒm 2020-2023 VÃ€ rating trÃªn 4 sao"
  â†’ {{ "operator": "AND", "conditions": [
       {{ "field": "TheLoai", "operator": "contains", "value": "VÄƒn há»c" }},
       {{ "field": "NamXuatBan", "operator": "gte", "value": 2020 }},
       {{ "field": "NamXuatBan", "operator": "lte", "value": 2023 }},
       {{ "field": "SoSao", "operator": "gte", "value": 4 }}
     ] }}

- "SÃ¡ch KHÃ”NG pháº£i giÃ¡o trÃ¬nh VÃ€ cÃ³ lÆ°á»£t mÆ°á»£n > 10"
  â†’ {{ "operator": "AND", "conditions": [
       {{ "field": "LoaiSach", "operator": "equals", "value": "GiaoTrinh", "negate": true }},
       {{ "field": "LuotMuon", "operator": "gte", "value": 10 }}
     ] }}

- "(SÃ¡ch cá»§a A HOáº¶C B) VÃ€ thá»ƒ loáº¡i vÄƒn há»c"
  â†’ {{ "operator": "AND", "conditions": [
       {{ "field": "TheLoai", "operator": "contains", "value": "VÄƒn há»c" }}
     ],
     "subQueries": [
       {{ "operator": "OR", "conditions": [
            {{ "field": "TacGia", "operator": "contains", "value": "A" }},
            {{ "field": "TacGia", "operator": "contains", "value": "B" }}
          ] }}
     ] }} h

LOGIC PHÃ‚N LOáº I (QUAN TRá»ŒNG):
- DÃ¹ng tim_sach_level_1: CÃ¢u há»i Ä‘Æ¡n giáº£n, 1 Ä‘iá»u kiá»‡n hoáº·c nhiá»u Ä‘iá»u kiá»‡n AND cÆ¡ báº£n (vÃ­ dá»¥: "SÃ¡ch vÄƒn há»c cá»§a A nÄƒm 2020")
- DÃ¹ng tim_sach_level_2: CÃ¢u há»i cÃ³ tá»« khÃ³a "HOáº¶C", "OR", "KHÃ”NG PHáº¢I", "NOT", hoáº·c nhiá»u Ä‘iá»u kiá»‡n lá»“ng nhau phá»©c táº¡p

CÃC Tá»ª KHÃ“A NHáº¬N DIá»†N LEVEL 2:
- "hoáº·c", "or", "hay"
- "khÃ´ng pháº£i", "not", "ngoáº¡i trá»«", "trá»«"
- "(... hoáº·c ...) vÃ  ...", "(... and ...) or ..."

OUTPUT (CHá»ˆ TRáº¢ Vá»€ JSON, KHÃ”NG CÃ“ TEXT KHÃC):
- Náº¿u lÃ  cÃ¢u há»i tÃ¬m sÃ¡ch Ä‘Æ¡n giáº£n: tráº£ vá» schema tim_sach_level_1
- Náº¿u lÃ  cÃ¢u há»i tÃ¬m sÃ¡ch phá»©c táº¡p (cÃ³ OR/NOT/nested): tráº£ vá» schema tim_sach_level_2
- Náº¿u lÃ  cÃ¢u há»i chung: {{ "intent": "general_info", "confidence": 0.9 }}

CHÃš Ã:
- Chá»‰ Ä‘iá»n giÃ¡ trá»‹ vÃ o cÃ¡c field cÃ³ thÃ´ng tin trong cÃ¢u há»i
- CÃ¡c field khÃ´ng liÃªn quan Ä‘á»ƒ null
- topList chá»‰ thÃªm khi user há»i "top", "xáº¿p háº¡ng", "nhiá»u nháº¥t"
- confidence > 0.7 thÃ¬ má»›i gá»i API backend

Báº®T Äáº¦U PHÃ‚N TÃCH:"""

    try:
        response = gemini_model.generate_content(prompt)
        text = response.text.strip()

        # Loáº¡i bá» markdown code block
        json_text = re.sub(r'```json\n?', '', text)
        json_text = re.sub(r'```\n?', '', json_text).strip()

        classification = json.loads(json_text)
        # print(f"ğŸ¯ Classification: {json.dumps(classification, ensure_ascii=False, indent=2)}")
        return classification

    except Exception as e:
        print(f"âš ï¸ Intent classification error: {str(e)}")
        return {
            "intent": "general_info",
            "confidence": 0.5
        }

# ===========================================
# HÃ€M Gá»ŒI BACKEND TIM_SACH_LEVEL_1
# ===========================================
def call_tim_sach_level_1(classification):
    """Gá»i API backend endpoint tim_sach_level_1"""
    try:
        url = f"{BACKEND_URL}/tim_sach_level_1"
        print(f"ğŸ“¡ Calling backend: {url}")
        print(f"ğŸ“¦ Payload: {json.dumps(classification, ensure_ascii=False, indent=2)}")

        response = requests.post(
            url,
            json=classification,
            headers={"Content-Type": "application/json"},
            timeout=15
        )

        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Backend returned {len(data) if isinstance(data, list) else 'N/A'} items")
            return data
        else:
            print(f"âŒ Backend error: {response.status_code} - {response.text}")
            return None

    except Exception as e:
        print(f"âŒ Backend call failed: {str(e)}")
        return None

# ===========================================
# HÃ€M Gá»ŒI BACKEND TIM_SACH_LEVEL_2
# ===========================================
def call_tim_sach_level_2(classification):
    """Gá»i API backend endpoint tim_sach_level_2"""
    try:
        url = f"{BACKEND_URL}/tim_sach_level_2"
        print(f"ğŸ“¡ Calling backend Level 2: {url}")
        print(f"ğŸ“¦ Payload: {json.dumps(classification, ensure_ascii=False, indent=2)}")

        response = requests.post(
            url,
            json=classification,
            headers={"Content-Type": "application/json"},
            timeout=15
        )

        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Backend Level 2 returned {len(data) if isinstance(data, list) else 'N/A'} items")
            return data
        else:
            print(f"âŒ Backend Level 2 error: {response.status_code} - {response.text}")
            return None

    except Exception as e:
        print(f"âŒ Backend Level 2 call failed: {str(e)}")
        return None

# ===========================================
# ROUTE Má»šI: SEMANTIC SEARCH (CHO BACKEND Gá»ŒI Láº I)
# ===========================================
@app.route("/semanticSearch", methods=["POST"])
def semantic_search():
    """
    Backend gá»i endpoint nÃ y khi tÃ¬m exact khÃ´ng ra káº¿t quáº£
    Payload: { "query": "text cáº§n tÃ¬m", "field": "TenSach|TacGia|...", "candidates": [...] }
    """
    global database_embeddings, database_texts

    try:
        data = request.json
        query = data.get("query", "").strip()
        field = data.get("field", "")
        candidates = data.get("candidates", [])

        if not query:
            return jsonify({
                "status": "error",
                "message": "Missing query parameter"
            }), 400

        print(f"ğŸ” Semantic search: query='{query}', field='{field}', candidates={len(candidates)}")

        # Náº¿u cÃ³ candidates tá»« backend
        if candidates:
            candidate_texts = [str(c) for c in candidates]
        else:
            # Fallback: tÃ¬m trong toÃ n bá»™ database_texts
            candidate_texts = database_texts

        if not candidate_texts:
            return jsonify({
                "status": "error",
                "message": "No candidates to search"
            }), 400

        # Encode query vÃ  candidates
        query_embedding = embedding_model.encode(query, convert_to_tensor=True)
        candidate_embeddings = embedding_model.encode(candidate_texts, convert_to_tensor=True)

        # TÃ­nh cosine similarity
        cos_scores = util.cos_sim(query_embedding, candidate_embeddings)[0]

        # Láº¥y top 5 káº¿t quáº£
        top_k = min(5, len(candidate_texts))
        top_results = torch.topk(cos_scores, k=top_k)

        results = []
        for score, idx in zip(top_results[0], top_results[1]):
            if score > 0.3:  # NgÆ°á»¡ng similarity
                results.append({
                    "text": candidate_texts[idx],
                    "score": float(score)
                })

        print(f"âœ… Found {len(results)} semantic matches")

        return jsonify({
            "status": "ok",
            "results": results
        })

    except Exception as e:
        print(f"âŒ Semantic search error: {str(e)}")
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

# ===========================================
# ROUTE 1: NHáº¬N DATABASE Tá»ª BACKEND
# ===========================================
@app.route("/sendDatabaseToColab", methods=["POST"])
def receive_database():
    global database_texts, database_embeddings

    try:
        data = request.json
        records = data.get("data", [])

        if not records:
            return jsonify({
                "status": "error",
                "message": "KhÃ´ng cÃ³ dá»¯ liá»‡u"
            }), 400

        print(f"ğŸ“¥ Nháº­n {len(records)} records tá»« backend")

        database_texts = [record.strip() for record in records if record.strip()]

        # Táº¡o embeddings
        print("â³ Äang táº¡o embeddings...")
        database_embeddings = embedding_model.encode(
            database_texts,
            convert_to_tensor=True,
            show_progress_bar=True
        )
        print(f"âœ… ÄÃ£ táº¡o {len(database_embeddings)} embeddings")
        print(f"   Shape: {database_embeddings.shape}")

        return jsonify({
            "status": "ok",
            "message": "ÄÃ£ nháº­n vÃ  embedding thÃ nh cÃ´ng",
            "total_records": len(records)
        })

    except Exception as e:
        print(f"âŒ Lá»—i: {str(e)}")
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

# ===========================================
# ROUTE 2: Xá»¬ LÃ CHAT (RAG)
# ===========================================
@app.route("/chatbot", methods=["POST"])
def chatbot():
    start_time = time.time()  # Báº®T Äáº¦U ÄO THá»œI GIAN

    global database_texts, database_embeddings

    try:
        data = request.json
        user_message = data.get("message", "").strip()

        if not user_message:
            return jsonify({
                "status": "error",
                "response": "Vui lÃ²ng nháº­p cÃ¢u há»i"
            }), 400

        print(f"ğŸ’¬ User: {user_message}")

        # ============================================
        # BÆ¯á»šC 1: PHÃ‚N LOáº I INTENT
        # ============================================
        classification = classify_intent(user_message)
        intent = classification.get("intent")
        confidence = classification.get("confidence", 0)

        print(f"ğŸ¯ Intent: {intent}, Confidence: {confidence}")

        # ============================================
        # BÆ¯á»šC 2: Xá»¬ LÃ TIM_SACH_LEVEL_1 (Gá»ŒI BACKEND)
        # ============================================
        if intent == "tim_sach_level_1" and confidence > 0.7:
            backend_data = call_tim_sach_level_1(classification)

            if backend_data:
                # Kiá»ƒm tra xem cÃ³ pháº£i weighted scoring khÃ´ng
                topList = classification.get("topList", [])
                is_weighted_scoring = topList and len(topList) > 1
                
                # Táº¡o prompt cho Gemini Ä‘á»ƒ format káº¿t quáº£ Ä‘áº¹p
                format_prompt = f"""Dá»±a vÃ o dá»¯ liá»‡u sau, hÃ£y tráº£ lá»i cÃ¢u há»i "{user_message}" má»™t cÃ¡ch tá»± nhiÃªn:

Dá»® LIá»†U:
{json.dumps(backend_data, ensure_ascii=False, indent=2)}

{"âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG: ÄÃ¢y lÃ  káº¿t quáº£ Tá»”NG Há»¢P (weighted scoring) dá»±a trÃªn NHIá»€U tiÃªu chÃ­ cÃ¹ng lÃºc. KHÃ”NG tÃ¡ch thÃ nh cÃ¡c danh sÃ¡ch riÃªng biá»‡t." if is_weighted_scoring else ""}

YÃŠU Cáº¦U FORMAT (TUÃ‚N THá»¦ NGHIÃŠM NGáº¶T):
- Tráº£ lá»i ngáº¯n gá»n, tá»± nhiÃªn nhÆ° Ä‘ang chat
{"- Náº¿u cÃ³ nhiá»u tiÃªu chÃ­ (lÆ°á»£t mÆ°á»£n + lÆ°á»£t xem + rating): ÄÃ‚Y LÃ€ Xáº¾P Háº NG Tá»”NG Há»¢P, chá»‰ liá»‡t kÃª 1 DANH SÃCH duy nháº¥t" if is_weighted_scoring else ""}
- Liá»‡t kÃª tá»«ng cuá»‘n sÃ¡ch báº±ng sá»‘ thá»© tá»±: 1., 2., 3.
- Má»—i cuá»‘n sÃ¡ch trÃ¬nh bÃ y theo format:
  Sá»‘ thá»© tá»±. TÃªn sÃ¡ch
  - MÃ£ sÃ¡ch: [mÃ£ sÃ¡ch]
  - TÃ¡c giáº£: [tÃªn tÃ¡c giáº£]
  - NÄƒm xuáº¥t báº£n: [nÄƒm]
  - [ThÃ´ng tin thÃªm: Náº¿u cÃ³ LuotMuon + LuotXem thÃ¬ PHáº¢I GHI Cáº¢ 2, náº¿u cÃ³ DanhGia thÃ¬ ghi cáº£ rating]

- TUYá»†T Äá»I KHÃ”NG dÃ¹ng dáº¥u ** (asterisk) Ä‘á»ƒ in Ä‘áº­m
- KHÃ”NG dÃ¹ng dáº¥u * á»Ÿ báº¥t ká»³ Ä‘Ã¢u
- Má»—i thÃ´ng tin xuá»‘ng dÃ²ng báº±ng gáº¡ch Ä‘áº§u dÃ²ng -
- Thá»¥t vÃ o 2 khoáº£ng tráº¯ng cho cÃ¡c dÃ²ng thÃ´ng tin chi tiáº¿t

VÃ Dá»¤ FORMAT ÄÃšNG (KHI CÃ“ NHIá»€U TIÃŠU CHÃ):
"ÄÃ¢y lÃ  top 3 sÃ¡ch cÃ³ lÆ°á»£t mÆ°á»£n vÃ  lÆ°á»£t xem cao nháº¥t (xáº¿p háº¡ng tá»•ng há»£p):

1. Máº¯t Biáº¿c
  - MÃ£ sÃ¡ch: S0003
  - TÃ¡c giáº£: Nguyá»…n Nháº­t Ãnh
  - NÄƒm xuáº¥t báº£n: 1990
  - LÆ°á»£t mÆ°á»£n: 15
  - LÆ°á»£t xem: 195

2. Ngá»“i KhÃ³c TrÃªn CÃ¢y
  - MÃ£ sÃ¡ch: S0008
  - TÃ¡c giáº£: Nguyá»…n Nháº­t Ãnh
  - NÄƒm xuáº¥t báº£n: 2013
  - LÆ°á»£t mÆ°á»£n: 13
  - LÆ°á»£t xem: 154

3. 1984
  - MÃ£ sÃ¡ch: S0006
  - TÃ¡c giáº£: George Orwell
  - NÄƒm xuáº¥t báº£n: 1949
  - LÆ°á»£t mÆ°á»£n: 17
  - LÆ°á»£t xem: 142"

âŒ TUYá»†T Äá»I KHÃ”NG TÃCH THÃ€NH:
"Top 3 sÃ¡ch mÆ°á»£n nhiá»u nháº¥t:
1. ...
2. ...

Top 3 sÃ¡ch xem nhiá»u nháº¥t:
1. ...
2. ..."

KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin khÃ´ng cÃ³ trong dá»¯ liá»‡u

TRáº¢ Lá»œI:"""

                try:
                    response = gemini_model.generate_content(format_prompt)
                    bot_response = response.text.strip()
                    print(f"âœ… Bot (from tim_sach_level_1): {bot_response[:100]}...")

                    # ChÃ¨n Ä‘o thá»i gian á»Ÿ Ä‘Ã¢y
                    end_time = time.time()
                    print(f"â±ï¸ Thá»i gian thá»±c hiá»‡n tim_sach_level_1: {end_time - start_time:.3f} giÃ¢y")

                    return jsonify({
                        "status": "ok",
                        "response": bot_response,
                        "source": "tim_sach_level_1"
                    })
                except Exception as e:
                    print(f"âŒ Format error: {str(e)}")
                    # Fallback xuá»‘ng RAG náº¿u format lá»—i
        # BÆ¯á»šC 2.5: Xá»¬ LÃ TIM_SACH_LEVEL_2 (Gá»ŒI BACKEND)
        # ============================================
        elif intent == "tim_sach_level_2" and confidence > 0.7:
            backend_data = call_tim_sach_level_2(classification)

            if backend_data:
                # Kiá»ƒm tra xem cÃ³ pháº£i weighted scoring khÃ´ng
                topList = classification.get("topList", [])
                is_weighted_scoring = topList and len(topList) > 1
                
                # Táº¡o prompt cho Gemini Ä‘á»ƒ format káº¿t quáº£ Ä‘áº¹p
                format_prompt = f"""Dá»±a vÃ o dá»¯ liá»‡u sau, hÃ£y tráº£ lá»i cÃ¢u há»i "{user_message}" má»™t cÃ¡ch tá»± nhiÃªn:

Dá»® LIá»†U:
{json.dumps(backend_data, ensure_ascii=False, indent=2)}

{"âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG: ÄÃ¢y lÃ  káº¿t quáº£ Tá»”NG Há»¢P (weighted scoring) dá»±a trÃªn NHIá»€U tiÃªu chÃ­ cÃ¹ng lÃºc. KHÃ”NG tÃ¡ch thÃ nh cÃ¡c danh sÃ¡ch riÃªng biá»‡t." if is_weighted_scoring else ""}

YÃŠU Cáº¦U FORMAT (TUÃ‚N THá»¦ NGHIÃŠM NGáº¶T):
- Tráº£ lá»i ngáº¯n gá»n, tá»± nhiÃªn nhÆ° Ä‘ang chat
{"- Náº¿u cÃ³ nhiá»u tiÃªu chÃ­ (lÆ°á»£t mÆ°á»£n + lÆ°á»£t xem + rating): ÄÃ‚Y LÃ€ Xáº¾P Háº NG Tá»”NG Há»¢P, chá»‰ liá»‡t kÃª 1 DANH SÃCH duy nháº¥t" if is_weighted_scoring else ""}
- Liá»‡t kÃª tá»«ng cuá»‘n sÃ¡ch báº±ng sá»‘ thá»© tá»±: 1., 2., 3.
- Má»—i cuá»‘n sÃ¡ch trÃ¬nh bÃ y theo format:
  Sá»‘ thá»© tá»±. TÃªn sÃ¡ch
  - MÃ£ sÃ¡ch: [mÃ£ sÃ¡ch]
  - TÃ¡c giáº£: [tÃªn tÃ¡c giáº£]
  - NÄƒm xuáº¥t báº£n: [nÄƒm]
  - [ThÃ´ng tin thÃªm náº¿u cÃ³: rating/lÆ°á»£t mÆ°á»£n/lÆ°á»£t xem/giÃ¡]

- TUYá»†T Äá»I KHÃ”NG dÃ¹ng dáº¥u ** (asterisk) Ä‘á»ƒ in Ä‘áº­m
- KHÃ”NG dÃ¹ng dáº¥u * á»Ÿ báº¥t ká»³ Ä‘Ã¢u
- Má»—i thÃ´ng tin xuá»‘ng dÃ²ng báº±ng gáº¡ch Ä‘áº§u dÃ²ng -
- Thá»¥t vÃ o 2 khoáº£ng tráº¯ng cho cÃ¡c dÃ²ng thÃ´ng tin chi tiáº¿t

VÃ Dá»¤ FORMAT ÄÃšNG:
"ÄÃ¢y lÃ  cÃ¡c cuá»‘n sÃ¡ch báº¡n cáº§n tÃ¬m:

1. Máº¯t Biáº¿c
  - MÃ£ sÃ¡ch: S0003
  - TÃ¡c giáº£: Nguyá»…n Nháº­t Ãnh
  - NÄƒm xuáº¥t báº£n: 1990
  - LÆ°á»£t mÆ°á»£n: 15
  - LÆ°á»£t xem: 195

2. NhÃ  Giáº£ Kim
  - MÃ£ sÃ¡ch: S0012
  - TÃ¡c giáº£: Paulo Coelho
  - NÄƒm xuáº¥t báº£n: 1988
  - LÆ°á»£t mÆ°á»£n: 8
  - LÆ°á»£t xem: 120"

KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin khÃ´ng cÃ³ trong dá»¯ liá»‡u

TRáº¢ Lá»œI:"""

                try:
                    response = gemini_model.generate_content(format_prompt)
                    bot_response = response.text.strip()
                    print(f"âœ… Bot (from tim_sach_level_2): {bot_response[:100]}...")

                    # ChÃ¨n Ä‘o thá»i gian á»Ÿ Ä‘Ã¢y
                    end_time = time.time()
                    print(f"â±ï¸ Thá»i gian thá»±c hiá»‡n tim_sach_level_2: {end_time - start_time:.3f} giÃ¢y")

                    return jsonify({
                        "status": "ok",
                        "response": bot_response,
                        "source": "tim_sach_level_2"
                    })
                except Exception as e:
                    print(f"âŒ Format error Level 2: {str(e)}")
                    # Fallback xuá»‘ng RAG náº¿u format lá»—i

        # ============================================
        # BÆ¯á»šC 3: FALLBACK - RAG THUáº¦N (CODE CÅ¨)
        # ============================================
        print("ğŸ”„ Falling back to pure RAG...")

        # Kiá»ƒm tra database Ä‘Ã£ Ä‘Æ°á»£c load chÆ°a
        if database_embeddings is None or len(database_texts) == 0:
            return jsonify({
                "status": "error",
                "response": "Há»‡ thá»‘ng chÆ°a sáºµn sÃ ng. Vui lÃ²ng Ä‘á»£i dá»¯ liá»‡u Ä‘Æ°á»£c táº£i lÃªn."
            }), 503

        # === TÃŒM CONTEXT LIÃŠN QUAN ===
        print("ğŸ” Äang tÃ¬m kiáº¿m context liÃªn quan...")
        query_embedding = embedding_model.encode(user_message, convert_to_tensor=True)

        # TÃ­nh cosine similarity
        cos_scores = util.cos_sim(query_embedding, database_embeddings)[0]

        # Láº¥y top 5 káº¿t quáº£ cÃ³ Ä‘iá»ƒm cao nháº¥t
        top_k = min(5, len(database_texts))
        top_results = torch.topk(cos_scores, k=top_k)

        print(f"\n{'='*80}")
        print(f"ğŸ’¬ CÃ‚U Há»I: {user_message}")
        print(f"{'='*80}")
        print(f"ğŸ“Š TOP {top_k} SIMILARITY SCORES:")
        for i, (score, idx) in enumerate(zip(top_results[0], top_results[1]), 1):
            print(f"\n[{i}] Score: {score:.4f}")
            print(f"    Text: {database_texts[idx][:200]}...")
        print(f"{'='*80}\n")

        relevant_contexts = []
        for score, idx in zip(top_results[0], top_results[1]):
            if score > 0.3:  # NgÆ°á»¡ng similarity
                relevant_contexts.append(database_texts[idx])
                print(f"   âœ“ Score {score:.3f}: {database_texts[idx][:100]}...")

        if not relevant_contexts:
            relevant_contexts = database_texts[:3]  # Fallback: láº¥y 3 Ä‘áº§u
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y context phÃ¹ há»£p, dÃ¹ng fallback")

        # === GENERATION - Táº O CÃ‚U TRáº¢ Lá»œI ===
        context_text = "\n\n".join(relevant_contexts)

        prompt = f"""Báº¡n lÃ  trá»£ lÃ½ áº£o thÃ´ng minh cá»§a thÆ° viá»‡n, nhiá»‡m vá»¥ tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn thÃ´ng tin cÃ³ sáºµn.

THÃ”NG TIN THÆ¯ VIá»†N:
{context_text}

CÃ‚U Há»I: {user_message}

YÃŠU Cáº¦U FORMAT:
- Tráº£ lá»i ngáº¯n gá»n, rÃµ rÃ ng, tá»± nhiÃªn
- Chia thÃ nh cÃ¡c cÃ¢u ngáº¯n, má»—i Ã½ xuá»‘ng dÃ²ng (dÃ¹ng \n)
- Náº¿u liá»‡t kÃª nhiá»u má»¥c, dÃ¹ng sá»‘ thá»© tá»±: 1., 2., 3.
- Náº¿u cÃ³ má»¥c con (phÃ¢n cáº¥p), dÃ¹ng:
  + Má»¥c cha: 1., 2., 3.
  + Má»¥c con: - (gáº¡ch Ä‘áº§u dÃ²ng), thá»¥t vÃ o 2 khoáº£ng tráº¯ng
- KHÃ”NG dÃ¹ng dáº¥u * (asterisk)
- KHÃ”NG Ä‘á»ƒ dÃ²ng trá»‘ng thá»«a giá»¯a cÃ¡c má»¥c
- Tá»± nhiÃªn nhÆ° Ä‘ang chat, khÃ´ng cá»©ng nháº¯c

KHÃ”NG ÄÆ¯á»¢C thÃªm thÃ´ng tin khÃ´ng cÃ³ trong pháº§n THÃ”NG TIN THÆ¯ VIá»†N
Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin: "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin vá» váº¥n Ä‘á» nÃ y ğŸ˜…"

TRáº¢ Lá»œI:"""

        print("ğŸ¤– Äang generate cÃ¢u tráº£ lá»i...")
        response = gemini_model.generate_content(prompt)
        bot_response = response.text.strip()

        print(f"âœ… Bot: {bot_response[:100]}...")

        return jsonify({
            "status": "ok",
            "response": bot_response,
            "source": "pure_rag"
        })

    except Exception as e:
        print(f"âŒ Lá»—i chatbot: {str(e)}")
        return jsonify({
            "status": "error",
            "response": "Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra. Vui lÃ²ng thá»­ láº¡i sau."
        }), 500

# ===========================================
# ROUTE 3: KIá»‚M TRA TRáº NG THÃI
# ===========================================
@app.route("/health", methods=["GET"])
def health_check():
    return jsonify({
        "status": "ok",
        "database_loaded": len(database_texts) > 0,
        "total_records": len(database_texts)
    })

# ===========================================
# BÆ¯á»šC 5: CHáº Y SERVER
# ===========================================
if __name__ == "__main__":
    # Táº¡o public URL qua ngrok
    public_url = ngrok.connect(5000)
    print("\n" + "="*60)
    print("ğŸš€ SERVER ÄANG CHáº Y!")
    print("="*60)
    print(f"ğŸ“¡ Public URL: {public_url}")
    print("="*60)
    print("\nâš ï¸ LÆ¯U Ã:")
    print("1. Copy URL trÃªn vÃ o file Vue cá»§a báº¡n")
    print("2. Thay tháº¿: 'https://kerchieft-crescentic-lavon.ngrok-free.dev'")
    print("3. Nhá»› thÃªm '/chatbot' hoáº·c '/semanticSearch' vÃ o cuá»‘i khi gá»i API\n")

    # Cháº¡y Flask
    app.run(port=5000)