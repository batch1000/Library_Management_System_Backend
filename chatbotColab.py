from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok
from sentence_transformers import SentenceTransformer, util
import torch
import google.generativeai as genai
import os

BACKEND_URL = "https://libraryphuongb2103514.pagekite.me/api/chatbot"

# C·∫•u h√¨nh Ngrok
!ngrok authtoken 35E1InvyKUj8F2iSLdU6N60wnLM_2DSMWqQTPd7gHXVDH3fC5

# C·∫•u h√¨nh Gemini API (L·∫§Y KEY T·∫†I: https://makersuite.google.com/app/apikey)
GEMINI_API_KEY = "AIzaSyDkzmIMw2CwvKwa2h4oMG-nNJSkSPUN2kY"  # ‚ö†Ô∏è THAY KEY C·ª¶A B·∫†N V√ÄO ƒê√ÇY
genai.configure(api_key=GEMINI_API_KEY)

# ===========================================
# B∆Ø·ªöC 3: KH·ªûI T·∫†O MODEL & BI·∫æN TO√ÄN C·ª§C
# ===========================================
print("‚è≥ ƒêang load embedding model...")
embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
print("‚úÖ Embedding model ƒë√£ s·∫µn s√†ng!")

# Bi·∫øn l∆∞u tr·ªØ database
database_texts = []
database_embeddings = None

# Kh·ªüi t·∫°o Gemini model
gemini_model = genai.GenerativeModel('models/gemini-2.5-flash')

# ===========================================
# B∆Ø·ªöC 4: T·∫†O FLASK APP
# ===========================================
app = Flask(__name__)
CORS(app, supports_credentials=True, resources={r"/*": {"origins": "*"}})

# H√†m ph√¢n lo·∫°i intent
def classify_intent(user_message):
    """Ph√¢n lo·∫°i c√¢u h·ªèi c·ªßa user b·∫±ng Gemini"""
    prompt = f"""B·∫°n l√† AI ph√¢n lo·∫°i c√¢u h·ªèi v·ªÅ th∆∞ vi·ªán.

NHI·ªÜM V·ª§: Ph√¢n t√≠ch c√¢u h·ªèi v√† tr·∫£ v·ªÅ JSON format.

C√ÇU H·ªéI: "{user_message}"

DANH S√ÅCH INTENT:
1. "top_ranking" - C√¢u h·ªèi v·ªÅ top/x·∫øp h·∫°ng/nhi·ªÅu nh·∫•t/cao nh·∫•t
   Sub-intents:
   - "most_borrowed": Top s√°ch nhi·ªÅu l∆∞·ª£t m∆∞·ª£n
   - "most_viewed": Top s√°ch nhi·ªÅu l∆∞·ª£t xem
   - "highest_rated": Top s√°ch rating cao nh·∫•t
   - "lowest_rated": Top s√°ch rating th·∫•p nh·∫•t
   - "newest": S√°ch m·ªõi nh·∫•t

2. "general_info" - Th√¥ng tin chung v·ªÅ th∆∞ vi·ªán, quy ƒë·ªãnh, gi·ªù m·ªü c·ª≠a...

3. "book_search" - T√¨m s√°ch c·ª• th·ªÉ theo t√™n/t√°c gi·∫£

OUTPUT (CH·ªà TR·∫¢ V·ªÄ JSON, KH√îNG C√ì TEXT KH√ÅC):
{{
  "intent": "top_ranking" | "general_info" | "book_search",
  "sub_intent": "most_borrowed" | "most_viewed" | "highest_rated" | "lowest_rated" | "newest" | null,
  "parameters": {{
    "limit": <s·ªë l∆∞·ª£ng, m·∫∑c ƒë·ªãnh 10>,
    "entity_type": "book"
  }},
  "confidence": <0.0-1.0>
}}

B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH:"""

    try:
        response = gemini_model.generate_content(prompt)
        text = response.text.strip()
        
        # Lo·∫°i b·ªè markdown code block
        import re
        json_text = re.sub(r'```json\n?', '', text)
        json_text = re.sub(r'```\n?', '', json_text).strip()
        
        import json
        classification = json.loads(json_text)
        print(f"üéØ Classification: {classification}")
        return classification
        
    except Exception as e:
        print(f"‚ö†Ô∏è Intent classification error: {str(e)}")
        return {
            "intent": "general_info",
            "sub_intent": None,
            "parameters": {},
            "confidence": 0.5
        }
import requests

def call_backend_filter(sub_intent, limit=10):
    """G·ªçi API backend local ƒë·ªÉ l·∫•y d·ªØ li·ªáu filtered"""
    
    # Map sub_intent sang t√™n h√†m backend
    endpoint_map = {
        "most_borrowed": "getTopBorrowedBooks",
        "most_viewed": "getTopViewedBooks",
        "highest_rated": "getTopRatedBooks",
        "lowest_rated": "getLowestRatedBooks",
        "newest": "getNewestBooks"
    }
    
    endpoint = endpoint_map.get(sub_intent)
    if not endpoint:
        return None
    
    try:
        url = f"{BACKEND_URL}/{endpoint}"
        print(f"üì° Calling backend: {url}")
        
        response = requests.get(url, params={"limit": limit}, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            print(f"‚úÖ Backend returned {len(data)} items")
            return data
        else:
            print(f"‚ùå Backend error: {response.status_code}")
            return None
            
    except Exception as e:
        print(f"‚ùå Backend call failed: {str(e)}")
        return None

# ===========================================
# ROUTE 1: NH·∫¨N DATABASE T·ª™ BACKEND
# ===========================================
@app.route("/sendDatabaseToColab", methods=["POST"])
def receive_database():
    global database_texts, database_embeddings

    try:
        data = request.json
        records = data.get("data", [])

        if not records:
            return jsonify({
                "status": "error",
                "message": "Kh√¥ng c√≥ d·ªØ li·ªáu"
            }), 400

        print(f"üì• Nh·∫≠n {len(records)} records t·ª´ backend")

        # print("\nüìö D·ªØ li·ªáu nh·∫≠n ƒë∆∞·ª£c t·ª´ backend:")
        # for i, record in enumerate(records, start=0):
        #   print(f"[{i}] {record}")

        database_texts = [record.strip() for record in records if record.strip()]


        # T·∫°o embeddings
        print("‚è≥ ƒêang t·∫°o embeddings...")
        database_embeddings = embedding_model.encode(
            database_texts,
            convert_to_tensor=True,
            show_progress_bar=True
        )
        print(f"‚úÖ ƒê√£ t·∫°o {len(database_embeddings)} embeddings")
        print(f"   Shape: {database_embeddings.shape}")

        return jsonify({
            "status": "ok",
            "message": "ƒê√£ nh·∫≠n v√† embedding th√†nh c√¥ng",
            "total_records": len(records)
        })

    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

# ===========================================
# ROUTE 2: X·ª¨ L√ù CHAT (RAG)
# ===========================================
@app.route("/chatbot", methods=["POST"])
def chatbot():
    global database_texts, database_embeddings

    try:
        data = request.json
        user_message = data.get("message", "").strip()

        if not user_message:
            return jsonify({
                "status": "error",
                "response": "Vui l√≤ng nh·∫≠p c√¢u h·ªèi"
            }), 400

        print(f"üí¨ User: {user_message}")

        # ============================================
        # B∆Ø·ªöC 1: PH√ÇN LO·∫†I INTENT
        # ============================================
        classification = classify_intent(user_message)
        intent = classification.get("intent")
        sub_intent = classification.get("sub_intent")
        confidence = classification.get("confidence", 0)
        
        print(f"üéØ Intent: {intent}, Sub: {sub_intent}, Confidence: {confidence}")

        # ============================================
        # B∆Ø·ªöC 2: X·ª¨ L√ù TOP_RANKING (G·ªåI BACKEND)
        # ============================================
        if intent == "top_ranking" and sub_intent and confidence > 0.6:
            limit = classification.get("parameters", {}).get("limit", 10)
            backend_data = call_backend_filter(sub_intent, limit)
            
            if backend_data:
                # T·∫°o prompt cho Gemini ƒë·ªÉ format k·∫øt qu·∫£ ƒë·∫πp
                format_prompt = f"""D·ª±a v√†o d·ªØ li·ªáu sau, h√£y tr·∫£ l·ªùi c√¢u h·ªèi "{user_message}" m·ªôt c√°ch t·ª± nhi√™n:

D·ªÆ LI·ªÜU:
{backend_data}

Y√äU C·∫¶U FORMAT (TU√ÇN TH·ª¶ NGHI√äM NG·∫∂T):
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n nh∆∞ ƒëang chat
- Li·ªát k√™ t·ª´ng cu·ªën s√°ch b·∫±ng s·ªë th·ª© t·ª±: 1., 2., 3.
- M·ªói cu·ªën s√°ch tr√¨nh b√†y theo format t∆∞∆°ng t·ª±:
  S·ªë th·ª© t·ª±. T√™n s√°ch
  - M√£ s√°ch: [m√£ s√°ch]
  - T√°c gi·∫£: [t√™n t√°c gi·∫£]
  - NƒÉm xu·∫•t b·∫£n: [nƒÉm]
  - [Th√¥ng tin th√™m n·∫øu c√≥: rating/l∆∞·ª£t m∆∞·ª£n/l∆∞·ª£t xem]

- TUY·ªÜT ƒê·ªêI KH√îNG d√πng d·∫•u ** (asterisk) ƒë·ªÉ in ƒë·∫≠m
- KH√îNG d√πng d·∫•u * ·ªü b·∫•t k·ª≥ ƒë√¢u
- M·ªói th√¥ng tin xu·ªëng d√≤ng b·∫±ng g·∫°ch ƒë·∫ßu d√≤ng -
- Th·ª•t v√†o 2 kho·∫£ng tr·∫Øng cho c√°c d√≤ng th√¥ng tin chi ti·∫øt

V√ç D·ª§ FORMAT ƒê√öNG:
"ƒê√¢y l√† top 3 s√°ch m·ªõi nh·∫•t nh√©:

1. Gi√°o Tr√¨nh V·∫≠t Li·ªáu X√¢y D·ª±ng
  - M√£ s√°ch: S003
  - T√°c gi·∫£: ThS. Phan Th·∫ø Vinh
  - NƒÉm xu·∫•t b·∫£n: 2011

2. Gi√°o Tr√¨nh Tri·∫øt H·ªçc M√°c - L√™Nin
  - M√£ s√°ch: S003
  - T√°c gi·∫£: B·ªô Gi√°o D·ª•c V√† ƒê√†o T·∫°o
  - NƒÉm xu·∫•t b·∫£n: 2000

3. Gi√°o Tr√¨nh Tr·∫Øc ƒê·ªãa Bi·ªÉn
  - M√£ s√°ch: S003
  - T√°c gi·∫£: TS ƒêinh Xu√¢n Vinh
  - NƒÉm xu·∫•t b·∫£n: 2003"

KH√îNG ƒë∆∞·ª£c th√™m th√¥ng tin kh√¥ng c√≥ trong d·ªØ li·ªáu
N·∫øu thi·∫øu th√¥ng tin rating/l∆∞·ª£t m∆∞·ª£n/l∆∞·ª£t xem th√¨ KH√îNG N√äN nh·∫Øc ƒë·∫øn

TR·∫¢ L·ªúI:"""
                
                try:
                    response = gemini_model.generate_content(format_prompt)
                    bot_response = response.text.strip()
                    print(f"‚úÖ Bot (from backend data): {bot_response[:100]}...")
                    
                    return jsonify({
                        "status": "ok",
                        "response": bot_response,
                        "source": "backend_filter"
                    })
                except Exception as e:
                    print(f"‚ùå Format error: {str(e)}")
                    # Fallback xu·ªëng RAG n·∫øu format l·ªói

        # ============================================
        # B∆Ø·ªöC 3: FALLBACK - RAG THU·∫¶N (CODE C≈®)
        # ============================================
        print("üîÑ Falling back to pure RAG...")
        
        # Ki·ªÉm tra database ƒë√£ ƒë∆∞·ª£c load ch∆∞a
        if database_embeddings is None or len(database_texts) == 0:
            return jsonify({
                "status": "error",
                "response": "H·ªá th·ªëng ch∆∞a s·∫µn s√†ng. Vui l√≤ng ƒë·ª£i d·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i l√™n."
            }), 503

        # === T√åM CONTEXT LI√äN QUAN ===
        print("üîç ƒêang t√¨m ki·∫øm context li√™n quan...")
        query_embedding = embedding_model.encode(user_message, convert_to_tensor=True)

        # T√≠nh cosine similarity
        cos_scores = util.cos_sim(query_embedding, database_embeddings)[0]

        # L·∫•y top 5 k·∫øt qu·∫£ c√≥ ƒëi·ªÉm cao nh·∫•t
        top_k = min(5, len(database_texts))
        top_results = torch.topk(cos_scores, k=top_k)

        print(f"\n{'='*80}")
        print(f"üí¨ C√ÇU H·ªéI: {user_message}")
        print(f"{'='*80}")
        print(f"üìä TOP {top_k} SIMILARITY SCORES:")
        for i, (score, idx) in enumerate(zip(top_results[0], top_results[1]), 1):
            print(f"\n[{i}] Score: {score:.4f}")
            print(f"    Text: {database_texts[idx][:200]}...")
        print(f"{'='*80}\n")

        relevant_contexts = []
        for score, idx in zip(top_results[0], top_results[1]):
            if score > 0.3:  # Ng∆∞·ª°ng similarity
                relevant_contexts.append(database_texts[idx])
                print(f"   ‚úì Score {score:.3f}: {database_texts[idx][:100]}...")

        if not relevant_contexts:
            relevant_contexts = database_texts[:3]  # Fallback: l·∫•y 3 ƒë·∫ßu
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y context ph√π h·ª£p, d√πng fallback")

        # === B∆Ø·ªöC 2: GENERATION - T·∫†O C√ÇU TR·∫¢ L·ªúI ===
        context_text = "\n\n".join(relevant_contexts)

        prompt = f"""B·∫°n l√† tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa th∆∞ vi·ªán, nhi·ªám v·ª• tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n th√¥ng tin c√≥ s·∫µn.

TH√îNG TIN TH∆Ø VI·ªÜN:
{context_text}

C√ÇU H·ªéI: {user_message}

Y√äU C·∫¶U FORMAT:
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng, t·ª± nhi√™n
- Chia th√†nh c√°c c√¢u ng·∫Øn, m·ªói √Ω xu·ªëng d√≤ng (d√πng \n)
- N·∫øu li·ªát k√™ nhi·ªÅu m·ª•c, d√πng s·ªë th·ª© t·ª±: 1., 2., 3.
- N·∫øu c√≥ m·ª•c con (ph√¢n c·∫•p), d√πng:
  + M·ª•c cha: 1., 2., 3.
  + M·ª•c con: - (g·∫°ch ƒë·∫ßu d√≤ng), th·ª•t v√†o 2 kho·∫£ng tr·∫Øng
- KH√îNG d√πng d·∫•u * (asterisk)
- KH√îNG ƒë·ªÉ d√≤ng tr·ªëng th·ª´a gi·ªØa c√°c m·ª•c
- T·ª± nhi√™n nh∆∞ ƒëang chat, kh√¥ng c·ª©ng nh·∫Øc

V√ç D·ª§ FORMAT ƒê√öNG:
"V·ªÅ ch·ª©c nƒÉng ni√™n lu·∫≠n c√≥ m·∫•y ƒëi·ªÉm sau:

1. Truy c·∫≠p: V√†o tab Th∆∞ vi·ªán ‚Üí ch·ªçn Ni√™n lu·∫≠n
2. Sinh vi√™n: Ch·ªçn m·ª•c N·ªôp ni√™n lu·∫≠n ƒë·ªÉ n·ªôp b√†i
3. Gi·∫£ng vi√™n c√≥ 2 m·ª•c ch√≠nh:
  - Qu·∫£n l√Ω ƒë·ª£t n·ªôp: t·∫°o ƒë·ª£t n·ªôp cho sinh vi√™n
  - Danh s√°ch ni√™n lu·∫≠n: xem to√†n b·ªô ni√™n lu·∫≠n trong khoa"

KH√îNG ƒê∆Ø·ª¢C th√™m th√¥ng tin kh√¥ng c√≥ trong ph·∫ßn TH√îNG TIN TH∆Ø VI·ªÜN
N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin: "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y üòÖ"


TR·∫¢ L·ªúI:"""

        print("ü§ñ ƒêang generate c√¢u tr·∫£ l·ªùi...")
        response = gemini_model.generate_content(prompt)
        bot_response = response.text.strip()

        print(f"‚úÖ Bot: {bot_response[:100]}...")

        return jsonify({
            "status": "ok",
            "response": bot_response
        })

    except Exception as e:
        print(f"‚ùå L·ªói chatbot: {str(e)}")
        return jsonify({
            "status": "error",
            "response": "Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i sau."
        }), 500

# ===========================================
# ROUTE 3: KI·ªÇM TRA TR·∫†NG TH√ÅI
# ===========================================
@app.route("/health", methods=["GET"])
def health_check():
    return jsonify({
        "status": "ok",
        "database_loaded": len(database_texts) > 0,
        "total_records": len(database_texts)
    })

# ===========================================
# B∆Ø·ªöC 5: CH·∫†Y SERVER
# ===========================================
if __name__ == "__main__":
    # T·∫°o public URL qua ngrok
    public_url = ngrok.connect(5000)
    print("\n" + "="*60)
    print("üöÄ SERVER ƒêANG CH·∫†Y!")
    print("="*60)
    print(f"üì° Public URL: {public_url}")
    print("="*60)
    print("\n‚ö†Ô∏è L∆ØU √ù:")
    print("1. Copy URL tr√™n v√†o file Vue c·ªßa b·∫°n")
    print("2. Thay th·∫ø: 'https://kerchieft-crescentic-lavon.ngrok-free.dev'")
    print("3. Nh·ªõ th√™m '/chatbot' v√†o cu·ªëi khi g·ªçi API\n")

    # Ch·∫°y Flask
    app.run(port=5000)